* faster
  #+BEGIN_HTML
    <div>
      <a href="https://crates.io/crates/faster">
        <img src="https://img.shields.io/crates/v/faster.svg" alt="crates.io" />
      </a>
      <a href="https://travis-ci.org/AdamNiederer/faster">
        <img src="https://travis-ci.org/AdamNiederer/faster.svg?branch=master" alt="Build Status"/>
      </a>
    </div>
  #+END_HTML

** SIMD for Humans
Easy, powerful, portable, absurdly fast numerical calculations. Includes static
dispatch with inlining based on your platform and vector types, zero-allocation
iteration, vectorized loading/storing, and support for uneven collections.

It looks something like this:
#+BEGIN_SRC rust
  let lots_of_3s = (&[-123.456f32; 128][..]).simd_iter()
      .simd_map(|v| { f32s::splat(9.0) * v.abs().sqrt().rsqrt().ceil().sqrt() -
                      f32s::splat(4.0) - f32s::splat(2.0) })
      .scalar_collect();
#+END_SRC

Which is analogous to this scalar code:
#+BEGIN_SRC rust
  let lots_of_3s = (&[-123.456f32; 128][..]).iter()
      .simd_map(|v| { 9.0 * v.abs().sqrt().sqrt().recip().ceil().sqrt() -
                      4.0 - 2.0 })
      .collect::<Vec<f32>>();
#+END_SRC

The vector size is entirely determined by the machine you're compiling for - it
attempts to use the largest vector size supported by your machine, and works on
any platform or architecture (see below for details).

Compare this to traditional explicit SIMD:
#+BEGIN_SRC rust
  use std::mem::transmute;
  use stdsimd::{f32x4, f32x8};

  let lots_of_3s = &mut [-123.456f32; 128][..];

  if cfg!(all(not(target_feature = "avx"), target_feature = "sse")) {
      for ch in init.chunks_mut(4) {
          let v = f32x4::load(ch, 0);
          let scalar_abs_mask = unsafe { transmute::<u32, f32>(0x7fffffff) };
          let abs_mask = f32x4::splat(scalar_abs_mask);
          // There isn't actually an absolute value intrinsic for floats - you
          // have to look at the IEEE 754 spec and do some bit flipping
          v = unsafe { _mm_and_ps(v, abs_mask) };
          v = unsafe { _mm_sqrt_ps(v) };
          v = unsafe { _mm_rsqrt_ps(v) };
          v = unsafe { _mm_ceil_ps(v) };
          v = unsafe { _mm_sqrt_ps(v) };
          v = unsafe { _mm_mul_ps(v, 9.0) };
          v = unsafe { _mm_sub_ps(v, 4.0) };
          v = unsafe { _mm_sub_ps(v, 2.0) };
          f32x4::store(ch, 0);
      }
  } else if cfg!(all(not(target_feature = "avx512"), target_feature = "avx")) {
      for ch in init.chunks_mut(8) {
          let v = f32x8::load(ch, 0);
          let scalar_abs_mask = unsafe { transmute::<u32, f32>(0x7fffffff) };
          let abs_mask = f32x8::splat(scalar_abs_mask);
          // There isn't actually an absolute value intrinsic for floats - you
          // have to look at the IEEE 754 spec and do some bit flipping
          v = unsafe { _mm256_and_ps(v, abs_mask) };
          v = unsafe { _mm256_sqrt_ps(v) };
          v = unsafe { _mm256_rsqrt_ps(v) };
          v = unsafe { _mm256_ceil_ps(v) };
          v = unsafe { _mm256_sqrt_ps(v) };
          v = unsafe { _mm256_mul_ps(v, 9.0) };
          v = unsafe { _mm256_sub_ps(v, 4.0) };
          v = unsafe { _mm256_sub_ps(v, 2.0) };
          f32x8::store(ch, 0);
      }
  }
#+END_SRC
Even with all of that boilerplate, this still only supports x86-64 machines with
SSE or AVX - and you have to look up each intrinsic to ensure it's usable for
your compilation target.
** Upcoming Features
Gathers and scatters are next in the pipeline. This should let you one-line
matrix determinants, cross products, and many vector calculus primitives.
** Compatibility
Faster currently supports x86 back to the first Pentium, although AVX-512
support isn't working in rustc yet. It builds on many architectures, although
I'm not sure whether the tests pass.
** Performance
Here are some extremely unscientific benchmarks which, at least, prove that this
isn't any worse than scalar iterators. Even on ancient CPUs, a lot of
performance can be extracted out of SIMD. Surprisingly, using SIMD iterators
performs better than scalar iterators even on the SSE-less Pentium.

However, intentionally worsening your program's locality for SIMD (as seen in
~tests::bench_determinmant2~ and ~tests::bench_determinant3~) is not a
worthwhile tradeoff unless you are doing a significant amount of work per
vector.

#+BEGIN_SRC shell
  $ RUSTFLAGS="-C target-cpu=ivybridge" cargo bench # host is ivybridge; target has AVX
  test tests::bench_determinant2_scalar ... bench:         426 ns/iter (+/- 8)
  test tests::bench_determinant2_simd   ... bench:         697 ns/iter (+/- 3)
  test tests::bench_determinant3_scalar ... bench:         352 ns/iter (+/- 5)
  test tests::bench_determinant3_simd   ... bench:         837 ns/iter (+/- 19)
  test tests::bench_map_scalar          ... bench:       6,965 ns/iter (+/- 49)
  test tests::bench_map_simd            ... bench:         899 ns/iter (+/- 14)
  test tests::bench_map_uneven_simd     ... bench:         904 ns/iter (+/- 5)
  test tests::bench_nop_scalar          ... bench:          39 ns/iter (+/- 0)
  test tests::bench_nop_simd            ... bench:          34 ns/iter (+/- 0)
  test tests::bench_reduce_scalar       ... bench:       6,883 ns/iter (+/- 33)
  test tests::bench_reduce_simd         ... bench:         873 ns/iter (+/- 2)
  test tests::bench_reduce_uneven_simd  ... bench:         902 ns/iter (+/- 5)
  test tests::bench_zip_nop_scalar      ... bench:         624 ns/iter (+/- 3)
  test tests::bench_zip_nop_simd        ... bench:         651 ns/iter (+/- 12)
  test tests::bench_zip_scalar          ... bench:         864 ns/iter (+/- 3)
  test tests::bench_zip_simd            ... bench:         852 ns/iter (+/- 25)

  RUSTFLAGS="-C target-cpu=x86-64" cargo bench # host is ivybridge; target has SSE2
  test tests::bench_determinant2_scalar ... bench:         392 ns/iter (+/- 2)
  test tests::bench_determinant2_simd   ... bench:         374 ns/iter (+/- 1)
  test tests::bench_determinant3_scalar ... bench:         358 ns/iter (+/- 2)
  test tests::bench_determinant3_simd   ... bench:         494 ns/iter (+/- 13)
  test tests::bench_map_scalar          ... bench:       7,195 ns/iter (+/- 49)
  test tests::bench_map_simd            ... bench:       1,873 ns/iter (+/- 10)
  test tests::bench_map_uneven_simd     ... bench:       1,886 ns/iter (+/- 7)
  test tests::bench_nop_scalar          ... bench:          38 ns/iter (+/- 0)
  test tests::bench_nop_simd            ... bench:          34 ns/iter (+/- 0)
  test tests::bench_reduce_scalar       ... bench:       7,006 ns/iter (+/- 24)
  test tests::bench_reduce_simd         ... bench:       1,863 ns/iter (+/- 4)
  test tests::bench_reduce_uneven_simd  ... bench:       1,932 ns/iter (+/- 10)
  test tests::bench_zip_nop_scalar      ... bench:         624 ns/iter (+/- 3)
  test tests::bench_zip_nop_simd        ... bench:         317 ns/iter (+/- 3)
  test tests::bench_zip_scalar          ... bench:         972 ns/iter (+/- 4)
  test tests::bench_zip_simd            ... bench:         495 ns/iter (+/- 2)

  $ RUSTFLAGS="-C target-cpu=pentium" cargo bench # host is ivybridge; this only runs the polyfills!
  test tests::bench_determinant2_scalar ... bench:         428 ns/iter (+/- 1)
  test tests::bench_determinant2_simd   ... bench:         394 ns/iter (+/- 3)
  test tests::bench_determinant3_scalar ... bench:         361 ns/iter (+/- 2)
  test tests::bench_determinant3_simd   ... bench:         598 ns/iter (+/- 4)
  test tests::bench_map_scalar          ... bench:       7,207 ns/iter (+/- 34)
  test tests::bench_map_simd            ... bench:       6,577 ns/iter (+/- 28)
  test tests::bench_map_uneven_simd     ... bench:       6,585 ns/iter (+/- 26)
  test tests::bench_nop_scalar          ... bench:          37 ns/iter (+/- 0)
  test tests::bench_nop_simd            ... bench:          65 ns/iter (+/- 0)
  test tests::bench_reduce_scalar       ... bench:       7,008 ns/iter (+/- 23)
  test tests::bench_reduce_simd         ... bench:       6,079 ns/iter (+/- 18)
  test tests::bench_reduce_uneven_simd  ... bench:       6,104 ns/iter (+/- 20)
  test tests::bench_zip_nop_scalar      ... bench:         626 ns/iter (+/- 4)
  test tests::bench_zip_nop_simd        ... bench:         289 ns/iter (+/- 2)
  test tests::bench_zip_scalar          ... bench:         975 ns/iter (+/- 6)
  test tests::bench_zip_simd            ... bench:         660 ns/iter (+/- 5)
#+END_SRC
