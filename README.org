* faster
  #+BEGIN_HTML
    <div>
      <a href="https://crates.io/crates/faster">
        <img src="https://img.shields.io/crates/v/faster.svg" alt="crates.io" />
      </a>
      <a href="https://travis-ci.org/AdamNiederer/faster">
        <img src="https://travis-ci.org/AdamNiederer/faster.svg?branch=master" alt="Build Status"/>
      </a>
    </div>
  #+END_HTML

** SIMD for Humans
Easy, powerful, portable, absurdly fast numerical calculations. Includes static
dispatch with inlining based on your platform and vector types, zero-allocation
iteration, vectorized loading/storing, and support for uneven collections.

It looks something like this:
#+BEGIN_SRC rust
  let lots_of_3s = (&[-123.456f32; 128][..]).simd_iter()
      .simd_map(|v| { f32s::splat(9.0) * v.abs().sqrt().rsqrt().ceil().sqrt() -
                      f32s::splat(4.0) - f32s::splat(2.0) })
      .scalar_collect();
#+END_SRC

Which is analogous to this scalar code:
#+BEGIN_SRC rust
  let lots_of_3s = (&[-123.456f32; 128][..]).iter()
      .simd_map(|v| { 9.0 * v.abs().sqrt().sqrt().recip().ceil().sqrt() -
                      4.0 - 2.0 })
      .collect::<Vec<f32>>();
#+END_SRC

The vector size is entirely determined by the machine you're compiling for - it
attempts to use the largest vector size supported by your machine, and works on
any platform or architecture (see below for details).

Compare this to traditional explicit SIMD:
#+BEGIN_SRC rust
  use std::mem::transmute;
  use stdsimd::{f32x4, f32x8};

  let lots_of_3s = &mut [-123.456f32; 128][..];

  if cfg!(all(not(target_feature = "avx"), target_feature = "sse")) {
      for ch in init.chunks_mut(4) {
          let v = f32x4::load(ch, 0);
          let scalar_abs_mask = unsafe { transmute::<u32, f32>(0x7fffffff) };
          let abs_mask = f32x4::splat(scalar_abs_mask);
          // There isn't actually an absolute value intrinsic for floats - you
          // have to look at the IEEE 754 spec and do some bit flipping
          v = unsafe { _mm_and_ps(v, abs_mask) };
          v = unsafe { _mm_sqrt_ps(v) };
          v = unsafe { _mm_rsqrt_ps(v) };
          v = unsafe { _mm_ceil_ps(v) };
          v = unsafe { _mm_sqrt_ps(v) };
          v = unsafe { _mm_mul_ps(v, 9.0) };
          v = unsafe { _mm_sub_ps(v, 4.0) };
          v = unsafe { _mm_sub_ps(v, 2.0) };
          f32x4::store(ch, 0);
      }
  } else if cfg!(all(not(target_feature = "avx512"), target_feature = "avx")) {
      for ch in init.chunks_mut(8) {
          let v = f32x8::load(ch, 0);
          let scalar_abs_mask = unsafe { transmute::<u32, f32>(0x7fffffff) };
          let abs_mask = f32x8::splat(scalar_abs_mask);
          // There isn't actually an absolute value intrinsic for floats - you
          // have to look at the IEEE 754 spec and do some bit flipping
          v = unsafe { _mm256_and_ps(v, abs_mask) };
          v = unsafe { _mm256_sqrt_ps(v) };
          v = unsafe { _mm256_rsqrt_ps(v) };
          v = unsafe { _mm256_ceil_ps(v) };
          v = unsafe { _mm256_sqrt_ps(v) };
          v = unsafe { _mm256_mul_ps(v, 9.0) };
          v = unsafe { _mm256_sub_ps(v, 4.0) };
          v = unsafe { _mm256_sub_ps(v, 2.0) };
          f32x8::store(ch, 0);
      }
  }
#+END_SRC
Even with all of that boilerplate, this still only supports x86-64 machines with
SSE or AVX - and you have to look up each intrinsic to ensure it's usable for
your compilation target.
** Upcoming Features
Gathers and scatters are next in the pipeline. This should let you one-line
matrix determinants, cross products, and many vector calculus primitives.
** Compatibility
Faster currently supports x86 back to the first Pentium, although AVX-512
support isn't working in rustc yet. It builds on many architectures, although
I'm not sure whether the tests pass.
** Performance
Here are some extremely unscientific benchmarks which, at least, prove that this
isn't any worse than scalar iterators. Even on ancient CPUs, a lot of
performance can be extracted out of SIMD.

#+BEGIN_SRC shell
  $ RUSTFLAGS="-C target-cpu=ivybridge" cargo bench # host is ivybridge
  test tests::bench_map_scalar         ... bench:         890 ns/iter (+/- 5)
  test tests::bench_map_simd           ... bench:         124 ns/iter (+/- 0)
  test tests::bench_map_uneven_simd    ... bench:         122 ns/iter (+/- 0)
  test tests::bench_nop_scalar         ... bench:          24 ns/iter (+/- 0)
  test tests::bench_nop_simd           ... bench:          24 ns/iter (+/- 0)
  test tests::bench_reduce_scalar      ... bench:         860 ns/iter (+/- 2)
  test tests::bench_reduce_simd        ... bench:         102 ns/iter (+/- 0)
  test tests::bench_reduce_uneven_simd ... bench:          99 ns/iter (+/- 0)

  $ RUSTFLAGS="-C target-cpu=pentium3" cargo bench # host is ivybridge
  test tests::bench_map_scalar         ... bench:         896 ns/iter (+/- 2)
  test tests::bench_map_simd           ... bench:         247 ns/iter (+/- 1)
  test tests::bench_map_uneven_simd    ... bench:         240 ns/iter (+/- 0)
  test tests::bench_nop_scalar         ... bench:          16 ns/iter (+/- 0)
  test tests::bench_nop_simd           ... bench:          18 ns/iter (+/- 0)
  test tests::bench_reduce_scalar      ... bench:         868 ns/iter (+/- 2)
  test tests::bench_reduce_simd        ... bench:         235 ns/iter (+/- 0)
  test tests::bench_reduce_uneven_simd ... bench:         222 ns/iter (+/- 0)
#+END_SRC
